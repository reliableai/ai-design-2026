## 5. Weekly Schedule (12 Weeks, 24 Lessons)

Each week has **two 2-hour lessons** (L1 & L2).  
The course has **three phases**:

- **Phase 1 (Lessons 1–3):** Building a simple AI app (agents, tools, memory, MCP-style connectors, optional voice).  
- **Phase 2 (Lessons 4–10):** Evaluation of AI systems (core theory + hands-on).  
- **Phase 3 (Lessons 11–24):** Project-driven work (reviews, meetings, presentations).

---

### Week 1 – Bootstrapping an AI-Powered App

**Lesson 1 – Hello Software 3.0 + OpenRouter Chat Service**  
**Lesson 2 – Tools & a Toy Agent Loop**

*(unchanged; see full syllabus for details)*

---

### Week 2 – Memory & Starting Evaluation

**Lesson 3 – Memory & MCP-Style Connectors (+ Optional Voice)**  
**Lesson 4 – Evaluation Fundamentals**

---

### Week 3 – Metrics & Golden Datasets I

**Lesson 5 – Metrics & Rating Schemes**  
**Lesson 6 – Golden Datasets I (Design & First Slice)**

---

### Week 4 – Golden Datasets II & LLM-as-Judge

**Lesson 7 – Golden Datasets II (Refinement & Peer Review)**  
**Lesson 8 – LLM-as-Judge & Evaluation Harness**

---

### Week 5 – Experimental Design, Error Analysis & Uncertainty

**Lesson 9 – Experimental Design & Ablations**  
**Lesson 10 – Error Analysis & Uncertainty**

---

### Week 6 – Architecture & Project Kick-off

**Lesson 11 – Architectures for AI Systems**  
**Lesson 12 – Track Selection & Project Specification**  
- Students choose **Track A** (class system) or **Track B** (capstone).  
- Teams formed, responsibilities defined.  
- **Project Spec v1** due.

---

### Week 7 – Midterm & First Project Review

> From this week on, **all contact hours are project-focused**: exams, project reviews, project meetings, or presentations. No new theory lectures.

**Lesson 13 – Midterm Written Exam (Individual)**  
- 90–120 minute written exam on material from Weeks 1–6:
  - Software 3.0, agents, tools, memory, MCP, voice.
  - Evaluation fundamentals: metrics, golden sets, LLM-as-judge, experiments, error analysis.

**Lesson 14 – Project Review Meeting #1 (Teams)**  
- Per-team review (Track A & B mixed):
  - Revisit problem statement & scope.  
  - Present refined architecture diagram.  
  - Present current evaluation plan and golden dataset status.  
- You and peers give feedback on:
  - Feasibility and focus.  
  - Evaluation coverage and realism.  
- Teams leave with a concrete **milestone plan** for Weeks 8–10.

---

### Week 8 – Deep Project Working Sessions

**Lesson 15 – Project Working Session #2 (Implementation Focus)**  
- Hands-on coding in class:
  - Integrate missing components (tools, memory, logging, safety modules).  
  - Ensure there is at least one end-to-end flow runnable.  
- Short 5-minute **stand-ups** per team:
  - What we did since last week.  
  - Blockers.  
  - Plan until next class.

**Lesson 16 – Project Working Session #3 (Evaluation Focus)**  
- Each team:
  - Finalizes first full **evaluation run** on the golden dataset.  
  - Debugs the evaluation harness as needed.  
- Mini “round-robin” reviews:
  - Teams pair up and quickly review each other’s eval setups & metrics.

---

### Week 9 – Mid-Project Presentations & Design Clinic

**Lesson 17 – Mid-Project Presentations (Internal Demo)**  
- 10–12 minutes per team:
  - Live or recorded demo of current system.  
  - Architecture overview.  
  - Current evaluation design and early results.  
  - Biggest open risks / unknowns.
- Feedback from instructor + peers on:
  - Clarity of design narrative.  
  - Strength of evaluation strategy.  
  - Suggestions for final stretch.

**Lesson 18 – Design & Evaluation Clinic #2 (Breakout Meetings)**  
- Breakout tables:
  - Table A: architecture & scalability questions.  
  - Table B: evaluation & golden dataset questions.  
  - Table C: safety/guardrails & memory behavior.  
- Teams rotate or camp at the table most relevant to their blockers.  
- Goal: unblock design/eval decisions for the final two weeks.

---

### Week 10 – Final Implementation Push

**Lesson 19 – Project Working Session #4 (Integration & Hardening)**  
- Focus on:
  - Integration between components (tools, memory, UI, eval harness).  
  - Error handling, logging, and basic observability.  
- Instructor circulates for 1:1 technical coaching.

**Lesson 20 – Project Working Session #5 (Final Evaluation Runs)**  
- Run **final evaluation suites**:
  - Golden dataset + main metrics.  
  - Key experiments/ablations (as defined in project spec).  
- Start drafting **results tables and plots** for the report.

---

### Week 11 – Final Reports & Presentation Prep

**Lesson 21 – Project Review Meeting #3 (Results & Storytelling)**  
- Each team brings:
  - Draft results section (tables/figures).  
  - Draft error taxonomy & key examples.  
- In-class:
  - You focus on sharpening the **narrative**:
    - What did we really learn?  
    - Which experiments mattered?  
    - How did evaluation change design?

**Lesson 22 – Presentation Rehearsals (Dry Runs)**  
- Teams rehearse their final presentation (~10–12 minutes).  
- Peer feedback on:
  - Clarity of slides/demos.  
  - Balance between design explanation and evaluation findings.  
  - Time management.

---

### Week 12 – Final Presentations & Retrospective

**Lesson 23 – Final Presentations (Graded)**  
- Each team presents:
  - System demo.  
  - Design choices & non-obvious alternatives considered.  
  - Evaluation framework and main results.  
  - Limitations & future work.
- Q&A after each presentation.

**Lesson 24 – Final Project Meeting & Course Retrospective**  
- Final project check-in:
  - Any last clarifications on reports (if due after class).  
- Whole-class retrospective:
  - What worked / didn’t in your system and process.  
  - How evaluation affected design decisions.  
  - How you would approach a **new** large-scale AI system differently after this course.

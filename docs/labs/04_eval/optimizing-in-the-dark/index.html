<!DOCTYPE html>
<!--
  Content by Fabio Casati
  https://www.linkedin.com/in/sphoebs/
  https://x.com/sphoebs
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimizing in the Dark: Organizational Blindness in AI Evaluations</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Mono:wght@400;500&family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;1,9..40,400&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'DM Sans', -apple-system, sans-serif;
            font-size: 17px;
            line-height: 1.7;
            max-width: 700px;
            margin: 0 auto;
            padding: 64px 24px;
            color: #2d3748;
            background: #f7fafc;
        }
        h1 {
            font-family: 'DM Sans', sans-serif;
            font-size: 2.4em;
            font-weight: 600;
            margin-bottom: 0.3em;
            letter-spacing: -0.025em;
            line-height: 1.15;
            color: #1a202c;
        }
        h2 {
            font-family: 'DM Sans', sans-serif;
            font-size: 1.5em;
            font-weight: 600;
            margin-top: 2.5em;
            margin-bottom: 0.6em;
            color: #2d3748;
            padding-bottom: 0.4em;
            border-bottom: 2px solid #38b2ac;
        }
        h3 {
            font-family: 'DM Mono', monospace;
            font-size: 1.05em;
            font-weight: 500;
            margin-top: 1.8em;
            color: #319795;
        }
        p {
            margin: 1.3em 0;
        }
        img {
            max-width: 70%;
            height: auto;
            margin: 2em auto;
            border-radius: 12px;
            display: block;
            border: 1px solid #e2e8f0;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        }
        blockquote {
            border-left: 4px solid #38b2ac;
            margin: 2em 0;
            padding: 1em 1.5em;
            color: #4a5568;
            background: #fff;
            border-radius: 0 8px 8px 0;
        }
        code {
            font-family: 'DM Mono', monospace;
            background: #e6fffa;
            padding: 0.2em 0.45em;
            border-radius: 6px;
            font-size: 0.88em;
            color: #234e52;
        }
        ul, ol {
            margin: 1.3em 0;
            padding-left: 1.6em;
        }
        li {
            margin: 0.6em 0;
        }
        hr {
            border: none;
            border-top: 1px solid #e2e8f0;
            margin: 2.5em 0;
        }
        a {
            color: #319795;
            text-decoration: underline;
        }
        a:hover {
            color: #285e5e;
        }
        strong {
            font-weight: 600;
            color: #1a202c;
        }
        em {
            font-style: italic;
            color: #4a5568;
        }
        footer {
            margin-top: 80px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            font-size: 0.9em;
            color: #a0aec0;
        }
    </style>
</head>
<body>
<h1>Optimizing in the Dark:<br>Organizational Blindness in AI Evaluations</h1>

<hr />
<p><em>A five-part series on the hidden uncertainty in AI evaluation metrics</em></p>
<hr />
<p>You've been in this room. A slide goes up. There's a metric—"Technical Accuracy", a number: 89%, and a color—green. The decision feels obvious.</p>
<img src="../figs/metric_meeting.png" alt="Meeting room with Technical Accuracy 89% slide" />
<p>But how reliable is that number? What if the true accuracy were 10+ points away from what you're seeing on the slide? What if the act of selecting the "best" system has systematically inflated your expectations? What if you're iterating toward a random target?</p>
<p>This series explores a structural problem in how organizations evaluate AI systems: we are building highly consequential systems, making decisions based on evaluation numbers, and systematically both <em>underestimating</em> and <em>ignoring</em> the bias and uncertainty in those numbers.</p>
<blockquote>
<p><strong>Notation (used throughout)</strong></p>
<ul>
<li><p>Scores are in <strong>percentage points</strong> ("points").</p></li>
<li><p><code>σ</code> = standard deviation of <strong>one-run measurement noise</strong> for a single system (random error).</p></li>
<li><p><code>±h</code> = approximate <strong>95% confidence interval half-width</strong> (so CI width is <code>2h</code>).</p></li>
<li><p>When comparing two systems with independent noise <code>σ</code>, the noise on the difference is <code>√2 · σ</code>.</p></li>
</ul>
</blockquote>
<hr />
<h2>The Series</h2>

<h3><a href="./part-1-structural-flaw.html">Part 1: A Structural Flaw in Judgment</a></h3>
<p>The problem statement. Why "89% accuracy" might be meaningless—or worse, misleading. The three facets of the problem: visibility, culture, and action.</p>

<h3><a href="./part-2-cost-of-ignorance.html">Part 2: The Cost of Ignorance</a></h3>
<p>The real costs of ignoring uncertainty: wasted cycles, misallocated portfolios, and production failures that were predictable all along. How bias and noise compound across organizations.</p>

<h3><a href="./part-3-value-of-better-measurement.html">Part 3: Better "Evals" Beats Better Dev</a></h3>
<p>The value of better measurement. The M×C matrix, the critical distinction between uncertainty and variability, and why better eval leads to better quality even without touching the system.</p>

<h3><a href="./part-4-sources-of-error.html">Part 4: Sources of Bias and Uncertainty</a></h3>
<p>The technical deep dive. From sample size effects to multiple hypothesis testing, from developer-induced overfitting to noisy LLM judges, from rubric mapping artifacts to prompt sensitivity. Each source alone can flip your decisions. Together, they compound.</p>

<h3><a href="./part-5-what-to-do.html">Part 5: What To Do, and What Not To Do</a></h3>
<p>The path forward. Addressing visibility (awareness, estimation, reporting), action (reducing uncertainty, building observability), and culture (naming things right, asking the questions, making accountability explicit). Practical tools including evaluation worksheets and AI-powered methodology assistants.</p>

<hr />
<h2>Key Themes</h2>
<ul>
<li><p><strong>Every score has two error terms</strong>: bias (systematic) and uncertainty (random). Bias is harder to see but often more dangerous.</p></li>
<li><p><strong>Selection turns noise into bias</strong>: Even unbiased evaluations become optimistic when you pick winners. The math is real.</p></li>
<li><p><strong>Small differences are noise</strong>: If per-system measurement noise is σ ≈ 4 points, a true 4-point gap still gives you a ~24% chance of picking the wrong one (independent Normal noise).</p></li>
<li><p><strong>The uncertainty is larger than you think</strong>: Sample size alone gives you a ~16-point-wide 95% confidence interval on 100 examples at 82% accuracy (≈82% ± 8). And that's the <em>optimistic</em> case.</p></li>
<li><p><strong>Better eval beats more dev effort</strong>: Investing in measurement first enables better deployment decisions and directed improvement—even before touching the agents.</p></li>
<li><p><strong>Culture matters</strong>: Organizations prefer harmony. Point estimates feel decisive. Uncertainty feels like weakness. But pretending certainty where none exists—<em>that's</em> weakness.</p></li>
</ul>
<hr />
<h2>Who This Is For</h2>
<ul>
<li><strong>Engineering leaders</strong> making ship/no-ship decisions based on evaluation metrics</li>
<li><strong>ML/AI practitioners</strong> designing and running evaluation pipelines</li>
<li><strong>Product managers</strong> interpreting and reporting quality numbers</li>
<li><strong>Executives</strong> who see scorecards and need to know what questions to ask</li>
</ul>
<hr />
<p><em>"Any measurement, without knowledge of the uncertainty, is meaningless."</em><br />
— Walter Lewin, MIT</p>
<hr />
<p><strong>Tags:</strong> <code>AI</code> <code>Machine Learning</code> <code>Evaluation</code> <code>MLOps</code> <code>AI Engineering</code> <code>Quality Assurance</code></p>
<footer>
    Content by Fabio Casati · <a href="https://www.linkedin.com/in/sphoebs/">LinkedIn</a> · <a href="https://x.com/sphoebs">X</a>
</footer>
</body>
</html>

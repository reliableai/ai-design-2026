<!DOCTYPE html>
<!--
  Content by Fabio Casati
  https://www.linkedin.com/in/sphoebs/
  https://x.com/sphoebs
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimizing in the Dark: Organizational Blindness in AI Evaluations</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Mono:wght@400;500&family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;1,9..40,400&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'DM Sans', -apple-system, sans-serif;
            font-size: 17px;
            line-height: 1.7;
            max-width: 700px;
            margin: 0 auto;
            padding: 64px 24px;
            color: #2d3748;
            background: #f7fafc;
        }
        h1 {
            font-family: 'DM Sans', sans-serif;
            font-size: 2.4em;
            font-weight: 600;
            margin-bottom: 0.3em;
            letter-spacing: -0.025em;
            line-height: 1.15;
            color: #1a202c;
        }
        h2 {
            font-family: 'DM Sans', sans-serif;
            font-size: 1.5em;
            font-weight: 600;
            margin-top: 2.5em;
            margin-bottom: 0.6em;
            color: #2d3748;
            padding-bottom: 0.4em;
            border-bottom: 2px solid #38b2ac;
        }
        h3 {
            font-family: 'DM Mono', monospace;
            font-size: 1.05em;
            font-weight: 500;
            margin-top: 1.8em;
            color: #319795;
        }
        p {
            margin: 1.3em 0;
        }
        img {
            max-width: 70%;
            height: auto;
            margin: 2em auto;
            border-radius: 12px;
            display: block;
            border: 1px solid #e2e8f0;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        }
        blockquote {
            border-left: 4px solid #38b2ac;
            margin: 2em 0;
            padding: 1em 1.5em;
            color: #4a5568;
            background: #fff;
            border-radius: 0 8px 8px 0;
        }
        blockquote p {
            margin: 0;
        }
        code {
            font-family: 'DM Mono', monospace;
            background: #e6fffa;
            padding: 0.2em 0.45em;
            border-radius: 6px;
            font-size: 0.88em;
            color: #234e52;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5em 0;
            font-size: 0.95em;
        }
        th, td {
            border: 1px solid #e2e8f0;
            padding: 10px 12px;
            text-align: left;
        }
        th {
            background: #edf2f7;
            font-weight: 600;
        }
        tr:nth-child(even) {
            background: #f7fafc;
        }
        ul, ol {
            margin: 1.3em 0;
            padding-left: 1.6em;
        }
        li {
            margin: 0.6em 0;
        }
        hr {
            border: none;
            border-top: 1px solid #e2e8f0;
            margin: 2.5em 0;
        }
        a {
            color: #319795;
            text-decoration: underline;
        }
        a:hover {
            color: #285e5e;
        }
        strong {
            font-weight: 600;
            color: #1a202c;
        }
        em {
            font-style: italic;
            color: #4a5568;
        }
        footer {
            margin-top: 80px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            font-size: 0.9em;
            color: #a0aec0;
        }
    </style>
</head>
<body>
<h1>Optimizing in the Dark:<br>Organizational Blindness in AI Evaluations</h1>

<h2>Part 1: A Structural Flaw in Judgment</h2>
<p><em>Iterating towards a random target</em></p>
<p><a href="./index.html">Series Index</a> | <a href="./part-2-cost-of-ignorance-draft.html">Part 2 →</a></p>
<hr />

<img src="./images/image1.png" alt="Technical Accuracy 89%" style="max-width: 49%;" />

<p>You have all been in this room. A slide goes up. There's a <em>metric</em>—"Technical Accuracy", a <em>number</em>: 89%, and a <em>color</em>—green.</p>

<p>Getting these metrics, numbers, and even the colors "right" are foundational to the success of a product.</p>

<p>They are the final output of a complex process of decisions and actions. This output is very consequential. Not only do they determine ship/no-ship decisions, but they tell engineers where to focus their energy, where to improve.
The metrics act like a <em>loss profile</em> and give us axes along which we need to improve our product. Unlike ML, this loss profile does not tell us how to improve, but it points us to what to work on.</p>

<img src="../figs/loss_profile.jpg" alt="Loss profile optimization landscape" />

<p>And as AI takes more of a leading role, they also tell AI where to improve—helping us make the "AI code generation → Evaluation" loop effective. Conversely, if we get these wrong, we iterate in the wrong directions, we ship things that make our customers lose trust in us, we hold back great features that could help us win deals.</p>

<img src="./images/image3.png" alt="AI-Human loop" />

<p>As I sit in the room at a customers' site and listen to presentation after presentation and report after report, I can't help but wonder: <em>how reliable are these numbers? Do the team reporting on the results know? And do we, and the executives sitting in the room, know? Do we grasp what that means for the decision we're about to make?</em></p>

<hr />

<p>Walter Lewin, the MIT physicist, in his first lecture of the course in basic Physics tells his students: "<em>Any measurement, without knowledge of the uncertainty, is meaningless.</em>" Not "less precise." Not "directionally useful." <strong>Meaningless.</strong> Then he adds: "<em>I want you to hear this at 3am tonight when you wake up.</em>" He says this <em>in the first lecture of the first course</em> on Physics, not at some random point.</p>

<p>And <strong>he was talking to people who live in the science of measurement</strong>.
We are not in that world. In enterprise AI we evaluate and report "at scale" across many use cases and teams, with vastly diverse audiences ranging from engineers to scientists to executives. It is a duty of the presenter to make sure that they convey information in a manner that enables the listeners to take the right action.</p>

<p>I posit that while the number in itself may be <em>meaningless</em>, the act of <strong>reporting</strong> without knowledge and indication of its uncertainty makes it <strong>misleading</strong>.</p>

<p>A common tendency in organizations is to wash this question away based on the idea that "<em>we are engineers, we are so cool because we approximate, and iterate. Dwelling on uncertainty is for theorists. A measure may not be perfect, we know, but it's a hint, and we will improve over time. We crawl, walk, run.</em>"
How many time have you heard this argument? Or how often do you hear that "yeah, it would be great to discuss uncertainty but executives won't understand, they need a simple number"?</p>

<p>But having a sense for how biased and noisy our measures can be is central to the notion of "engineering approximation". More specifically:</p>

<blockquote>
<p><em>A measurement - and a report of a measurement - is harmful if it leads me to make the wrong decision, or take the wrong action on a system.</em></p>
</blockquote>

<p>Now, this is an issue only if it is true that 1. AI measures and reports are characterized by significant uncertainty, and 2. we don't communicate it or, even worse, we are not even aware that this is something to worry about.</p>

<hr />

<h2>The Anthropic Paper and the Industry's Response</h2>

<p>I started paying closer attention to this problem when Anthropic published a paper titled "<a href="https://arxiv.org/abs/2411.00640">Adding Error Bars to Evals</a>." I thought: "It's about time people in this industry begin making this point."</p>

<img src="./images/image4.png" alt="Anthropic paper" />

<p>But..... no, I was wrong. Social media were telling a different story.
Pedro Domingos said: "<em>Spectacular breakthrough in AI: Anthropic has discovered error bars!</em>" The sarcasm was widespread. "<em>What an unserious field</em>", people said, that needs to be told to add error bars.</p>

<img src="./images/image5.png" alt="Social media reactions" />

<p><strong>The mockery proved the paper's point.</strong> (the paper is on way more than "error bars" - and uncertainty is not synonymous with "error bars" - but it makes the point).
If this was obvious and practiced, the paper wouldn't exist. The fact that a leading AI lab felt the need to write it—and the fact that the community's response was ridicule rather than embarrassment—tells us where we are.</p>

<hr />

<h1>Kahneman's "Noise" and Organizational Blindness</h1>

<p>Around the same time, I came across Kahneman, Sibony, and Sunstein's book "<em>Noise: A Flaw in Human Judgment</em>."</p>

<p>One of its central observations is that <em>organizations systematically underestimate—and resist acknowledging—variability</em> in judgment. We prefer consensus and harmony. We don't want to see the noise. The book focuses more on human judgment and assessment of human judgment, but the same applies to assessment of AI systems.</p>

<p>This book resonated well with my experience working with many companies and system integrators.</p>

<img src="./images/image6.jpeg" alt="Noise book" />

<p>Even more insightful was the experience I had from discussing it with a friend working at an AI company in the Valley. His response after reading it was: "<em>Yeah, the book is pretty useless. We know there's a standard deviation.</em>" This is the same kind of response that the Anthropic article received.</p>

<p>The point the book is making is not that assessments are subject to errors. But that such errors are large, and largely ignored by organizations. The book dwell then some more on how orgs are especially insensitive to noise even more so than bias - for us, they are both relevant and need to be addressed.</p>

<p>So, yes, there's noise, and variance—and we don't know:</p>

<ul>
<li>How large it is,</li>
<li>Where it comes from,</li>
<li>How much it's costing us,</li>
<li>How to reduce it</li>
<li><em>Or even how to talk about it</em></li>
</ul>

<p>And - noise and variance are only part of the problem. In most of the companies I have worked with, Measurement processes can be—and typically are—<em>systematically</em> biased: consistently producing results that are too optimistic or simply measuring the wrong thing.</p>

<p>What Kahneman helped me see is that this isn't just a skills gap. It's <strong>structural</strong>. Organizations have never had to rigorously measure the quality of machine judgment and have never had to deal with measurements that have so much uncertainty and this level of complexity. And to do so at scale.
The muscle doesn't exist—and it is unclear even if the right incentives to develop it are in place.</p>

<hr />

<h2>The Problem Statement</h2>

<p>This is the problem I want to talk about: we are building highly consequential AI systems, making decisions based on evaluation numbers, and we are systematically both <em>underestimating</em> and <em>ignoring</em> the uncertainty in those numbers.</p>

<p>This begs the question: who bears responsibility for this widespread flaw? Is it the person presenting? The person who designed the reporting template? The team that delivered evaluation tools? The instructors that prepared the training material? or, <em>The executive who doesn't ask? The culture that punishes uncertainty?</em> Or, the natural unwillingness of organizations to accept the existence of variability within a structured process?</p>

<p>The question is hard also because the "problem" landscape has three distinct facets:</p>

<ol>
<li><p><strong>The visibility problem</strong> (epistemic): People don't see the uncertainty. It's not reported, not computed, not surfaced. The green 89% looks solid. Decision-makers can't account for what they don't know exists.</p></li>
<li><p><strong>The culture problem</strong> (organizational): Even when people could see uncertainty, the organization doesn't ask. We don't reward quantifying it. We don't penalize ignoring it. The question "how confident are we?" isn't part of the discussion process. Uncertainty stays invisible because making it visible has no upside and potential downside (looking less confident, slowing things down).</p></li>
<li><p><strong>The action problem</strong> (methodological): Even when uncertainty is visible and the culture asks about it, people don't know what to do. How do you decide when the range is 77–95%? How do you reduce variance when you don't know which source dominates? The frameworks and practices are neither widely known nor well-established in gen AI.</p></li>
</ol>

<p>Let's understand the cost of inaction—and then what to do about it.</p>

<hr />

<p><em>Next: <a href="./part-2-cost-of-ignorance-draft.html">Part 2: The Cost of Ignorance</a> — Why ignoring uncertainty is expensive</em></p>

<hr />

<p><strong>Tags:</strong> <code>AI</code> <code>Machine Learning</code> <code>Evaluation</code> <code>MLOps</code> <code>AI Engineering</code></p>

<footer>
    Content by Fabio Casati · <a href="https://www.linkedin.com/in/sphoebs/">LinkedIn</a> · <a href="https://x.com/sphoebs">X</a>
</footer>
</body>
</html>

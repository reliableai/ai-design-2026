<!DOCTYPE html>
<!--
  Content by Fabio Casati
  https://www.linkedin.com/in/sphoebs/
  https://x.com/sphoebs
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Designing Agentic Systems: Axes, Abstractions, and MCP</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Instrument+Serif:ital@0;1&family=JetBrains+Mono:wght@400;500&family=Sora:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <link rel="stylesheet" href="../../lecture-style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
</head>
<body>

<nav>
    <div class="logo">Agentic Systems Design</div>
    <ul>
        <li><a href="ai_tools_integration.html">Part 1</a></li>
        <li><a href="aoa.html">Part 2</a></li>
        <li><a href="#context">Context</a></li>
        <li><a href="#axes">Axes</a></li>
        <li><a href="#abstractions">Abstractions</a></li>
        <li><a href="#mcp">MCP</a></li>
        <li><a href="#proposal">Proposal</a></li>
    </ul>
</nav>

<main>

    <!-- TITLE -->
    <div class="title-section" id="intro">
        <h1>Designing Agentic Systems</h1>
        <p class="subtitle">Roles, Axes, Abstractions, and the Role of MCP</p>
    </div>

    <!-- CONTEXT -->
    <section id="context">
        <div class="section-number">05 / CONTEXT</div>
        <h2>Context and Motivation</h2>

        <p>AI agents bring <strong>new opportunities</strong> and <strong>new failure modes</strong>.</p>

        <p>However, AI agents are only <em>partly</em> different from humans. Because of this, we have a great deal to learn from what already works well — most notably, <strong>the Web</strong>.</p>

        <p>A key part of the design challenge is therefore to:</p>
        <ul>
            <li>identify what is <strong>similar</strong> to prior systems,</li>
            <li>identify what is <strong>genuinely new</strong>,</li>
            <li>and understand how both opportunities and failures change as a result.</li>
        </ul>

        <p>Once we do this, we can reason more clearly about:</p>
        <ul>
            <li>useful <strong>abstractions</strong>,</li>
            <li>necessary <strong>middleware</strong>,</li>
            <li>and eventually, meaningful <strong>standardizations</strong> — things we need to agree on as a community.</li>
        </ul>

        <p>We also have lessons from Web services and SOA. But we must be mindful of a crucial difference:</p>

        <blockquote>
            In Web services and SOA, the <em>consumer of specifications was human</em> — a consumer that is, on average, <strong>far less capable and far less careful</strong> than a modern LLM at reading, interpreting, and consuming specifications.
        </blockquote>

        <p>This changes both the <strong>power</strong> and the <strong>risk profile</strong> of agentic systems.</p>
    </section>

    <!-- AXES -->
    <section id="axes">
        <div class="section-number">06 / AXES</div>
        <h2>Core Design Axes</h2>

        <div class="divider"></div>

        <!-- AXIS 1 -->
        <h3>Axis 1 — Who Reads What (and How Interaction Is Guided)</h3>

        <p>This axis combines two questions:</p>
        <ol>
            <li><strong>Who consumes the service or tool description?</strong></li>
            <li><strong>What description and context do they consume — and with what guidance?</strong></li>
        </ol>

        <p>Possible consumers include:</p>
        <ul>
            <li>Human users (via web pages)</li>
            <li>Developers (via API documentation)</li>
            <li>LLM-based agents</li>
            <li>Hybrid human + agent setups</li>
        </ul>

        <h4>A. New capability: LLMs as specification consumers</h4>

        <p>LLMs can read and digest specifications at a scale and level of detail that humans typically cannot:</p>
        <ul>
            <li>they can ingest long and complex descriptions,</li>
            <li>consistently follow mechanical constraints and schemas,</li>
            <li>cross-reference multiple sources,</li>
            <li>and operate "carefully" in the sense of not skipping steps or forgetting details.</li>
        </ul>

        <p>We can actually specify more behaviors and constratints than we did for humans (or maybe we had them in Terms and Conditions - now LLMs can read those, too)</p>

        <div class="thesis-block">
            <div class="label">Key Question</div>
            <p>We had "specs" kind of working for the web: do we need something different here?</p>
        </div>

        <h4>B. New weakness: agents are sometimes given <em>less</em> context than humans</h4>

        <p>Despite this increased capability, many agent tool ecosystems — including MCP-style descriptions — risk providing <em>less</em> context than classic web experiences.</p>

        <p>On the Web, users are surrounded by rich and redundant context:</p>
        <ul>
            <li>information architecture and navigation</li>
            <li>page structure and visual hierarchy</li>
            <li>explanations, examples, caveats</li>
            <li>implicit "you are here" state</li>
        </ul>

        <p>By contrast, tool descriptions often consist of:</p>
        <ul>
            <li>an operation name</li>
            <li>a JSON schema</li>
            <li>a short textual description</li>
            <li>little or no global view of the service</li>
        </ul>

        <p>LLMs can consume much more context — yet we sometimes give them much less.</p>

        <div class="thesis-block">
            <div class="label">Open Question</div>
            <p>What is the equivalent of the "whole site" and its structure when exposing capabilities to agents?</p>
        </div>

        <h4>C. Missing ingredient (or is it an opportunity to capture?): guided, step-by-step interaction</h4>

        <p>Web applications do not merely expose capabilities; they <strong>guide users through safe sequences</strong>.</p>

        <p>For example, an Amazon purchase flow is not a single action, but a guided interaction:</p>
        <ul>
            <li>cart review</li>
            <li>shipping selection</li>
            <li>payment selection</li>
            <li>explicit confirmation</li>
            <li>post-action receipts and state</li>
        </ul>

        <p>In many MCP- or tool-based integrations, how do we do this? do we one-shot it, always? Do we leave it to the client / agent?</p>

        <div class="content-card">
            <h4>Open problem</h4>
            <p>Assuming we know what we want to "support", how do we express and enforce guided flows for agents:</p>
            <ul>
                <li>what the next valid actions are,</li>
                <li>when confirmation is required,</li>
                <li>what must be shown to users,</li>
                <li>and what constitutes meaningful consent?</li>
            </ul>
        </div>

        <div class="divider"></div>

        <!-- AXIS 2 -->
        <h3>Axis 2 — Decision and Authority (and Guardrails / Controls)</h3>

        <p><strong>who controls which decisions?</strong>, and <strong>how that control is expressed, enforced, and audited?</strong>.</p>

        <p>(And: control rarely sits in one place. <strong>It is typically split across client, provider, and sometimes middleware.</strong>)</p>

        <h4>A. "Acting" is a bundle of decisions</h4>

        <p>Even a seemingly simple action contains multiple decision points:</p>
        <ul>
            <li><strong>Interpretation</strong>: what does the user mean?</li>
            <li><strong>Planning</strong>: what sequence is acceptable?</li>
            <li><strong>Tool selection</strong>: which capability is appropriate and allowed?</li>
            <li><strong>Parameter choice</strong>: which values, thresholds, and scope?</li>
            <li><strong>Commitment</strong>: when does it become irreversible?</li>
            <li><strong>Recovery</strong>: what happens on partial failure (retry, compensate, escalate)?</li>
            <li><strong>Disclosure</strong>: what must be shown to the user before proceeding?</li>
        </ul>

        <blockquote>For each decision point: who controls it, and how do we prevent skipping or guessing?</blockquote>

        <h4>B. Guardrails are not "extra safety" — they define behavior under uncertainty</h4>

        <p>Many "agent failures" are not model failures, but control failures:</p>
        <ul>
            <li>committing too early,</li>
            <li>filling missing parameters with guesses,</li>
            <li>suppressing errors,</li>
            <li>skipping required steps,</li>
            <li>presenting approvals that are not meaningful.</li>
        </ul>

        <p>Guardrails are therefore part of the system's <strong>behavioral contract</strong>, not a bolt-on.</p>

        <div class="thesis-block">
            <div class="label">Design Tension</div>
            <p>Which constraints must be enforceable (hard), and which can only be guidance (soft)? And where can each realistically live?</p>
        </div>

        <h4>C. Control lives across layers (and the split should be intentional)</h4>

        <p>In practice, different layers have different strengths:</p>

        <ul>
            <li><strong>Clients/agents</strong> tend to control: intent elicitation, interaction policy (ask vs infer), step-by-step UX, user-facing summaries, recovery strategy.</li>
            <li><strong>Providers/services</strong> tend to control: permissions, scopes, budgets, validation, refusal on ambiguity, commitment boundaries, receipts.</li>
            <li><strong>Middleware/platforms</strong> often emerge to control: centralized policy, cross-tool enforcement, consent lifecycle, tracing, anomaly detection.</li>
        </ul>

        <div class="thesis-block">
            <div class="label">Key Question</div>
            <p>What should providers specify and how? Which abstractions, "standards" or best practices should we adopt? Should a new kind of middleware emerge to support this, and where should it reside? How can client side abstractions help client avoid pitfalls? What should we monitor? What can be learned quickly, at scale, from a community of clients usign a service?</p>
        </div>

        <h4>D. The autonomy leash</h4>

        <p>A useful mental model is a leash as a <strong>vector of constraints</strong>, not a single "low/medium/high autonomy" knob:</p>

        <ul>
            <li>scope (which objects/resources)</li>
            <li>action class (read vs write vs transfer vs irreversible)</li>
            <li>risk tier (financial, privacy, safety, reputational)</li>
            <li>budgets (cost/time/volume/rate)</li>
            <li>data boundaries (what can be accessed/returned)</li>
            <li>reversibility (compensation possible?)</li>
            <li>time window (authority expiration)</li>
            <li>confirmation policy (what requires explicit approval, and when)</li>
        </ul>

        <blockquote>Where is each constraint declared, where is it enforced, and where is it merely suggested?</blockquote>

        <p>This also changes over time: in code, we have the notion of commit, and autonomy lives commit to commit (now). Are there similar business abstractions?</p>

        <h4>E. Observability is part of control (not an afterthought)</h4>

        <p>Guardrails that cannot be observed are hard to trust, hard to debug, and hard to govern. So observability is not "logging later" — it is part of the control design.</p>

        <p>A useful distributed-systems framing is to treat one agent run as a <strong>trace</strong>:</p>
        <ul>
            <li>the overall agent interaction is the trace</li>
            <li>each tool call (including MCP tool invocations) is a <strong>span</strong></li>
            <li>approvals / confirmations / refusals / commits are <strong>events</strong> (or spans, depending on granularity)</li>
            <li>"receipts" are the durable record of what actually changed</li>
        </ul>

        <p>This points naturally to <strong>OpenTelemetry</strong> (or something like it) as a baseline for <em>plumbing</em>:</p>
        <ul>
            <li>propagate trace context end-to-end (client → middleware → provider)</li>
            <li>correlate downstream service calls with the originating agent action</li>
            <li>enable cross-tool debugging when something goes wrong (or silently fails)</li>
        </ul>

        <p>But "using traces" is not enough. We also need <strong>semantic conventions</strong>:</p>
        <ul>
            <li>what does a "tool invocation" mean?</li>
            <li>what event marks a "commit point"?</li>
            <li>how do we represent "approval requested" vs "approval granted"?</li>
            <li>how do we represent "refusal due to ambiguity" vs "refusal due to policy"?</li>
            <li>what constitutes a receipt?</li>
        </ul>

        <p>MCP matters here in a specific way:</p>
        <ul>
            <li>it gives us a structured invocation surface where trace context <em>can</em> be attached and propagated,</li>
            <li>but it does not, by itself, define which semantic events, receipts, or approval artifacts should exist.</li>
        </ul>

        <div class="content-card">
            <h4>Privacy tension</h4>
            <p>And then the obvious tension:</p>
            <ul>
                <li>traces can easily capture prompts, user data, and sensitive content</li>
                <li>so we also need conventions around redaction, access control, and retention:
                    <ul>
                        <li>what is safe to store?</li>
                        <li>who gets to see it?</li>
                        <li>what can be shared across org boundaries?</li>
                    </ul>
                </li>
            </ul>
        </div>

        <div class="divider"></div>

        <!-- AXIS 3 -->
        <h3>Axis 3 — Consent, Approvals, Terms (and Responsibility)</h3>

        <p>This axis is where legal and social reality enters the system design.</p>

        <p>Consent is not a checkbox.<br>
        Approvals are not a modal dialog.<br>
        Terms and conditions are not "some PDF somewhere" where the real assumption is that nobody reads them.</p>

        <p>They define:</p>
        <ul>
            <li>what authority exists,</li>
            <li>what disclosures were required,</li>
            <li>what was agreed to (and when),</li>
            <li>and ultimately, who is responsible for outcomes.</li>
        </ul>

        <h4>A. What counts as meaningful consent?</h4>

        <p>In web flows, consent and approvals are embedded in interaction:</p>
        <ul>
            <li>the UI constrains what can happen,</li>
            <li>the user sees a concrete summary at commitment,</li>
            <li>the system records what was done.</li>
        </ul>

        <p>In agentic flows, it is easy to degrade consent into something that is neither meaningful to users nor defensible later.</p>

        <p>So we need to ask:</p>

        <blockquote>
            What must be shown to the user before approval?<br>
            What does the user approve: a plan, a summary, a specific action, a delegation?<br>
            How do we avoid "consent fatigue" while keeping consent real?
        </blockquote>

        <h4>B. Approvals and terms should become <em>artifacts</em>, not vibes</h4>

        <p>A practical stance is to treat approvals/consent/terms as first-class artifacts:</p>
        <ul>
            <li>what was approved (stable summary)</li>
            <li>under which scope and thresholds</li>
            <li>for how long (expiry / revocation)</li>
            <li>under which terms/policy version</li>
            <li>with what identity / principal</li>
        </ul>

        <p>Otherwise we end up with:</p>
        <ul>
            <li>"approvals" that feel meaningful but are technically meaningless, or</li>
            <li>controls that are technically enforceable but legally indefensible.</li>
        </ul>

        <p>The problem again is who can help where. What can providers do to help? what can frameworks do to help? what can standards do to help?</p>

        <h4>C. Responsibility is shared — but the boundaries must be explicit</h4>

        <p>A recurring failure mode is that responsibility is implicit until failure, and then becomes contested.</p>

        <p>So the question is:</p>

        <blockquote>
            When something goes wrong, what was preventable by the provider, what was preventable by the client, and what required middleware?<br>
            What evidence do we have (receipts, traces, consent artifacts) to support that answer?
        </blockquote>

    </section>

    <!-- ABSTRACTIONS -->
    <section id="abstractions">
        <div class="section-number">07 / ABSTRACTIONS</div>
        <h2>The Abstractions We Need</h2>

        <div class="content-card">
            <h4>1. Capability Abstraction (What Can Be Done)</h4>
            <p><strong>Missing today:</strong> preconditions, side effects, cost, risk, reversibility</p>
            <p><strong>Needed outcome:</strong> Agents (and humans) must be able to reason about <em>world impact</em>, not just API calls.</p>
        </div>

        <div class="content-card">
            <h4>2. Intent Abstraction (Why It Is Done)</h4>
            <p><strong>Missing today:</strong> explicit representation of: user intent, inferred agent intent, scope and expiration</p>
            <p><strong>Needed outcome:</strong> Intent must be a <strong>first-class object</strong>, inspectable and auditable.</p>
        </div>

        <div class="content-card">
            <h4>3. Control / Autonomy Contract (Who May Do What, When)</h4>
            <p>We need an explicit way to represent the "leash vector":</p>
            <ul>
                <li>scope, action class, risk tier</li>
                <li>budgets and data boundaries</li>
                <li>confirmation policy</li>
                <li>expiry and revocation</li>
            </ul>
            <p>And we need it to be:</p>
            <ul>
                <li>readable by clients/agents (for planning and UX),</li>
                <li>enforceable by providers/middleware (for safety/cost),</li>
                <li>and loggable for audit.</li>
            </ul>
        </div>

        <div class="content-card">
            <h4>4. Guided Interaction / Flow Semantics (How to Proceed Safely)</h4>
            <p>If the Web's superpower is guided flows, we need an equivalent abstraction:</p>
            <ul>
                <li>state ("where are we?")</li>
                <li>allowed next actions</li>
                <li>required confirmations and disclosures</li>
                <li>commitment points</li>
                <li>recovery semantics</li>
            </ul>
            <p>Otherwise "tools" stay as isolated calls, and safety/UX becomes ad hoc.</p>
        </div>

        <div class="content-card">
            <h4>5. Observability with Meaning (What Happened, and Why)</h4>
            <p>Beyond logs, we need:</p>
            <ul>
                <li><strong>semantic traces</strong>: intent → plan → actions → outcomes</li>
                <li><strong>business assertions</strong>: invariants and postconditions that detect silent failure</li>
                <li><strong>receipts</strong>: structured, auditable outcomes (what changed, where, under what authority)</li>
            </ul>
        </div>

        <div class="content-card">
            <h4>6. Consent / Approval / Terms Artifacts (Authority Basis)</h4>
            <p>We need standard ways to represent:</p>
            <ul>
                <li>approvals (what was approved, by whom, when)</li>
                <li>consent grants (scope, thresholds, expiry, revocation)</li>
                <li>terms/policy versions referenced at commitment</li>
            </ul>
            <p>These artifacts connect the technical system to legal accountability.</p>
        </div>

    </section>

    <!-- MCP -->
    <section id="mcp">
        <div class="section-number">08 / MCP</div>
        <h2>Where MCP Helps (and Where It Does Not)</h2>

        <p>A useful way to think about MCP is that it standardizes a <strong>tool invocation boundary</strong>.</p>

        <p>Specifically, MCP standardizes:</p>
        <ul>
            <li>how capabilities exposed by a <em>given server</em> can be enumerated,</li>
            <li>how those capabilities are invoked (inputs / outputs),</li>
            <li>and the structure of the interaction between a client/agent and that server.</li>
        </ul>

        <p>This is <strong>local, explicit discovery</strong>:</p>
        <ul>
            <li>the client already knows <em>which server</em> it is talking to,</li>
            <li>MCP does not provide global search or semantic service discovery,</li>
            <li>it does not answer "who can do X?", only "what can this server do?".</li>
        </ul>

        <p>This standardized boundary is important because it creates a <strong>stable interposition point</strong>: a place where additional logic <em>can</em> sit between client and service.</p>

        <p>Examples of what may be interposed here include:</p>
        <ul>
            <li>policy enforcement (permissions, quotas, budgets),</li>
            <li>logging and trace propagation,</li>
            <li>rate limiting and throttling,</li>
            <li>approval or consent checks,</li>
            <li>request/response validation.</li>
        </ul>

        <p>MCP itself does not define these behaviors. It merely provides a consistent surface where they can be implemented.</p>

        <p>This is meaningful — but it is not the same as standardizing <strong>meaning</strong>, <strong>flows</strong>, <strong>authority</strong>, or <strong>responsibility</strong>.</p>

        <div class="thesis-block">
            <div class="label">Mental Model</div>
            <p>MCP is to agentic tool ecosystems what HTTP was to the Web: foundational, enabling — and insufficient on its own.</p>
        </div>

        <h3>What MCP helps with</h3>

        <ul>
            <li><strong>Discovery + invocation consistency</strong>
                <ul>
                    <li>a uniform way for clients/agents to find tools and call them</li>
                    <li>structured inputs/outputs reduces "stringly typed" prompt glue</li>
                </ul>
            </li>
            <li><strong>A natural mediation and governance choke point</strong>
                <ul>
                    <li>a place where policy checks <em>can</em> be inserted</li>
                    <li>a place where rate limits / budgets <em>can</em> be enforced (depending on architecture)</li>
                </ul>
            </li>
            <li><strong>A place to attach observability plumbing</strong>
                <ul>
                    <li>MCP calls map naturally to spans ("tool call as span")</li>
                    <li>trace context can be propagated across boundaries (client → tool server → downstream services)</li>
                    <li>this is where OpenTelemetry-style propagation becomes practical</li>
                </ul>
            </li>
            <li><strong>Potentially: richer descriptions than raw APIs</strong>
                <ul>
                    <li>MCP descriptions <em>could</em> include guidance, examples, risk metadata, and "what this is for"</li>
                    <li>whether we actually do that (and standardize it) is still unclear</li>
                </ul>
            </li>
        </ul>

        <h3>What MCP does not solve</h3>

        <p>MCP does not, by itself, solve the core problems highlighted by the axes:</p>

        <ul>
            <li><strong>Axis 1 (Who reads what / guidance):</strong>
                <ul>
                    <li>MCP does not guarantee that agents get "whole site" context</li>
                    <li>MCP does not define how to guide step-by-step interactions</li>
                </ul>
            </li>
            <li><strong>Axis 2 (Control allocation / autonomy leash):</strong>
                <ul>
                    <li>MCP does not define an autonomy contract (scope, risk tiers, confirmation policy)</li>
                    <li>MCP does not define enforceable commitment points (preview vs commit)</li>
                    <li>MCP does not define recovery/compensation semantics</li>
                </ul>
            </li>
            <li><strong>Axis 3 (Consent, terms, responsibility):</strong>
                <ul>
                    <li>MCP does not define what constitutes meaningful consent</li>
                    <li>MCP does not define approval/terms artifacts (what was approved, under which version, by whom)</li>
                    <li>MCP does not resolve responsibility boundaries when failures occur</li>
                </ul>
            </li>
            <li><strong>Observability with meaning:</strong>
                <ul>
                    <li>MCP can carry trace context, but does not define semantic conventions: what counts as "commit"? what is a "receipt"? what does "refusal" mean (policy vs ambiguity vs missing consent)?</li>
                    <li>without shared conventions, traces remain technically useful but semantically shallow</li>
                </ul>
            </li>
        </ul>

        <h3>The real question MCP raises</h3>

        <p>Not "do we adopt MCP?", but:</p>

        <blockquote>What do we standardize <em>on top of</em> MCP so that tool calling becomes governable?</blockquote>

        <p>Candidate targets (not yet solved by MCP):</p>
        <ul>
            <li>guided interaction / flow semantics</li>
            <li>preview → commit patterns</li>
            <li>receipts and business assertions</li>
            <li>consent / approval / terms artifacts</li>
            <li>semantic trace conventions (OpenTelemetry-compatible)</li>
        </ul>

        <div class="divider"></div>

        <h3>Synthesis</h3>

        <p>AI agents are systems that <strong>read</strong>, <strong>decide</strong>, and <strong>act</strong>.</p>

        <p>The challenge is not only "how do we call tools?" It is making explicit:</p>
        <ul>
            <li>what is read and what context is available,</li>
            <li>who controls which decisions and how constraints are enforced,</li>
            <li>what constitutes consent under which terms,</li>
            <li>and how responsibility is assigned and evidenced.</li>
        </ul>

        <p>This is where abstractions, middleware, and (eventually) standardization matter.</p>

    </section>

    <!-- PROPOSAL -->
    <section id="proposal">
        <div class="section-number">09 / PROPOSAL</div>
        <h2>Proposal: Abstractions, Practices, and "Where They Live" for Governable Agentic Tool Ecosystems (MCP-compatible)</h2>

        <h3>Motivation</h3>

        <p>Agentic systems are distributed systems that can <strong>read</strong>, <strong>decide</strong>, and <strong>act</strong>. The core risk is not "models are dumb"; it's that:</p>
        <ul>
            <li>authority is implicit,</li>
            <li>commitments are blurred,</li>
            <li>failures are silent,</li>
            <li>and responsibility becomes a debate after the fact.</li>
        </ul>

        <div class="thesis-block">
            <div class="label">Goal</div>
            <p>Define a small set of abstractions and practices that make agentic systems <strong>governable at scale</strong> — and be explicit about where each abstraction should be <strong>declared</strong>, <strong>enforced</strong>, and <strong>evidenced</strong>.</p>
        </div>

        <p>A useful framing is three planes:</p>
        <ul>
            <li><strong>Data plane</strong>: actions that change state (tools/services)</li>
            <li><strong>Control plane</strong>: authority, guardrails, policy</li>
            <li><strong>Evidence plane</strong>: traces, receipts, approvals</li>
        </ul>

        <p>MCP is best viewed as a standardized boundary in the data plane that can carry control and evidence metadata.</p>

        <div class="divider"></div>

        <h3>Design principle: "Declare / Enforce / Evidence" (often different places)</h3>

        <p>For each abstraction, we should be able to answer:</p>
        <ol>
            <li><strong>Where is it declared?</strong> (who states the rule/meaning)</li>
            <li><strong>Where is it enforced?</strong> (who can actually prevent violation)</li>
            <li><strong>Where is it evidenced?</strong> (what artifacts prove what happened)</li>
        </ol>

        <p><strong>If we can't answer these, we don't have a system; we have vibes.</strong></p>

        <div class="divider"></div>

        <h3>Core abstractions to develop</h3>

        <div class="content-card">
            <h4>1) Capability semantics (meaning, not just schema)</h4>
            <p><strong>What:</strong> preconditions, side effects, reversibility, risk tier, cost model, common failure modes.</p>
            <ul>
                <li><strong>Declared by:</strong> provider (they know semantics)</li>
                <li><strong>Enforced by:</strong> provider (hard bounds) + middleware (cross-tool policy)</li>
                <li><strong>Evidenced by:</strong> receipts + traces</li>
            </ul>
            <p><strong>MCP fit:</strong> MCP can carry this as structured metadata alongside tool descriptions, but MCP does not define the schema for "meaning".</p>
        </div>

        <div class="content-card">
            <h4>2) Action classes + risk tiers (shared vocabulary)</h4>
            <p><strong>What:</strong> minimal taxonomy like:</p>
            <ul>
                <li>action class: <code>read</code>, <code>write</code>, <code>transfer</code>, <code>irreversible</code>, <code>safety_sensitive</code></li>
                <li>risk category: <code>financial</code>, <code>privacy</code>, <code>safety</code>, <code>reputational</code></li>
                <li>risk level: <code>low/med/high</code> (or numeric)</li>
            </ul>
            <ul>
                <li><strong>Declared by:</strong> provider per operation</li>
                <li><strong>Used by:</strong> clients/agents to decide confirmations and UI summaries</li>
                <li><strong>Enforced by:</strong> provider/middleware (thresholds, scopes), client (UX gating)</li>
            </ul>
            <p><strong>Why this matters:</strong> it's the cheapest "standardization" that unlocks consistent guardrails, monitoring, and consent.</p>
        </div>

        <div class="content-card">
            <h4>3) Commitment points: Preview → Commit (two-phase)</h4>
            <p><strong>What:</strong> make irreversible actions explicit and hard to bypass.</p>
            <ul>
                <li><code>preview()</code> returns:
                    <ul>
                        <li>stable summary of effects (what the user would approve)</li>
                        <li>a <code>confirm_token</code> (or commit reference)</li>
                        <li>optional computed plan / price / constraints</li>
                    </ul>
                </li>
                <li><code>commit(confirm_token, …)</code> performs the irreversible step
                    <ul>
                        <li>must be idempotent (idempotency key)</li>
                        <li>must return a receipt</li>
                    </ul>
                </li>
            </ul>
            <ul>
                <li><strong>Declared by:</strong> provider (commit semantics are domain truth)</li>
                <li><strong>Orchestrated by:</strong> client/agent (step-by-step flow + summaries)</li>
                <li><strong>Enforced by:</strong> provider (cannot commit without token / policy)</li>
                <li><strong>Evidenced by:</strong> commit event + receipt</li>
            </ul>
            <p><strong>MCP fit:</strong> MCP makes it easy to expose both calls consistently; it does not mandate the pattern.</p>
        </div>

        <div class="content-card">
            <h4>4) Consent / delegation artifacts (authority objects)</h4>
            <p><strong>What:</strong> consent is an object, not a chat message:</p>
            <ul>
                <li>principal (who authorized)</li>
                <li>scope (what resources)</li>
                <li>thresholds (limits)</li>
                <li>expiry / revocation</li>
                <li>terms/policy version references (see next)</li>
            </ul>
            <ul>
                <li><strong>Created by:</strong> client UX and/or middleware consent service</li>
                <li><strong>Validated by:</strong> provider at commit boundary (must be checkable)</li>
                <li><strong>Stored by:</strong> middleware/platform (often) or provider (if account owner)</li>
                <li><strong>Evidenced by:</strong> consent ID referenced in receipts + traces</li>
            </ul>
            <p><strong>Why:</strong> if consent isn't an artifact, you can't reason about responsibility or disputes.</p>
        </div>

        <div class="content-card">
            <h4>5) Terms / policy artifacts (what rules applied)</h4>
            <p><strong>What:</strong> treat "terms & conditions" and policy as versioned, referenceable inputs to authority:</p>
            <ul>
                <li><code>terms_version</code>, <code>policy_version</code></li>
                <li>what disclosures were shown (at least as a stable reference)</li>
                <li>which version was accepted for this action/authority grant</li>
            </ul>
            <ul>
                <li><strong>Published by:</strong> provider/platform</li>
                <li><strong>Presented by:</strong> client (at the moment it matters)</li>
                <li><strong>Enforced by:</strong> provider/middleware at commit (must reference a valid version)</li>
                <li><strong>Evidenced by:</strong> acceptance artifact + receipt fields + trace events</li>
            </ul>
            <p>This is the bridge between "technical correctness" and "legal defensibility".</p>
        </div>

        <div class="content-card">
            <h4>6) Guided interaction / flow semantics (the "whole website" problem)</h4>
            <p><strong>What:</strong> the missing web affordance: state + allowed next steps + required confirmations.</p>
            <p>Two practical flavors:</p>
            <ul>
                <li><strong>Provider flow hints</strong>: "if the user wants X, typical safe sequence is …"</li>
                <li><strong>Client-owned orchestration</strong>: client chooses UX, but must respect constraints and commit points</li>
            </ul>
            <ul>
                <li><strong>Declared by:</strong> provider as hints + hard constraints; client as UX flow</li>
                <li><strong>Enforced by:</strong> provider at commitment points; middleware for cross-tool rules</li>
                <li><strong>Evidenced by:</strong> trace shows step progression; receipts show commits</li>
            </ul>
            <p><strong>Non-goal:</strong> providers shouldn't own everyone's UX.<br>
            <strong>Goal:</strong> providers should expose enough structure that clients don't have to guess.</p>
        </div>

        <div class="content-card">
            <h4>7) Receipts (durable outcomes)</h4>
            <p><strong>What:</strong> every state-changing action returns a structured receipt:</p>
            <ul>
                <li>what changed (object IDs, deltas or stable summary)</li>
                <li>under what authority basis (consent/delegation reference)</li>
                <li>when, where, correlation IDs</li>
                <li>reversible? compensation path?</li>
            </ul>
            <ul>
                <li><strong>Produced by:</strong> provider (source of truth)</li>
                <li><strong>Stored by:</strong> client + middleware for audit/support</li>
                <li><strong>Evidenced by:</strong> receipts are the evidence</li>
            </ul>
            <p><strong>Without receipts, responsibility becomes storytelling.</strong></p>
        </div>

        <div class="content-card">
            <h4>8) Semantic traces (OpenTelemetry + conventions)</h4>
            <p><strong>What:</strong> tracing plumbing + a shared semantic layer:</p>
            <ul>
                <li>one agent run = <strong>trace</strong></li>
                <li>each tool/MCP invocation = <strong>span</strong></li>
                <li>approvals, refusals, preview, commit, compensation = <strong>events</strong> or spans</li>
                <li>receipts referenced by ID</li>
            </ul>
            <ul>
                <li><strong>Implemented by:</strong> all layers (client/middleware/provider)</li>
                <li><strong>Standardized as:</strong> OpenTelemetry-compatible semantic conventions (community)</li>
                <li><strong>Privacy controls:</strong> redaction, access control, retention (must be explicit)</li>
            </ul>
            <p>This matters because: guardrails without observability are unverifiable, "success" without meaning is misleading, silent failures dominate at scale.</p>
        </div>

        <div class="content-card">
            <h4>9) Business assertions (postconditions / invariants)</h4>
            <p><strong>What:</strong> detect "successful nonsense":</p>
            <ul>
                <li>invariant checks (domain truth)</li>
                <li>postconditions ("intended outcome holds")</li>
                <li>statistical assertions (system-level drift)</li>
            </ul>
            <ul>
                <li><strong>Declared by:</strong> provider (domain invariants) + sometimes client (intent invariants)</li>
                <li><strong>Evaluated by:</strong> provider + middleware</li>
                <li><strong>Evidenced by:</strong> assertion outcomes in traces/metrics and receipts/errors</li>
            </ul>
        </div>

        <div class="divider"></div>

        <h3>Where these should live (quick residence map)</h3>

        <p><strong>Clients/agents</strong> should own:</p>
        <ul>
            <li>intent elicitation, interaction policy (ask vs infer)</li>
            <li>step-by-step UX and user-facing summaries</li>
            <li>collecting approvals and producing consent artifacts (or calling a consent service)</li>
            <li>propagating trace context, emitting semantic events</li>
        </ul>

        <p><strong>Providers/services</strong> should own:</p>
        <ul>
            <li>capability semantics metadata (as much as feasible)</li>
            <li>enforceable constraints: permissions, scope, budgets, rate limits</li>
            <li>commitment boundaries (preview/commit) and refusal on ambiguity at commit time</li>
            <li>receipts as a first-class output</li>
        </ul>

        <p><strong>Middleware/platform</strong> often must exist to own:</p>
        <ul>
            <li>cross-tool policy (one place to encode org rules)</li>
            <li>consent lifecycle (issuance, expiry, revocation)</li>
            <li>trace pipeline defaults (redaction, retention, access control)</li>
            <li>anomaly detection + automated analysis loops</li>
        </ul>

        <p><strong>A healthy system uses all three intentionally.</strong></p>

        <div class="divider"></div>

        <h3>Practices to develop (not optional at scale)</h3>

        <h4>Provider practices</h4>
        <ul>
            <li>publish action class + risk tier per capability</li>
            <li>require preview/commit for irreversible/high-risk actions</li>
            <li>refuse ambiguous requests at commit boundaries (structured refusal reasons)</li>
            <li>always return receipts for state changes</li>
            <li>version semantics (meaning changes are breaking changes)</li>
            <li>agentic misuse-case design (ambiguity, escalation bypass, consent laundering)</li>
        </ul>

        <h4>Client/agent practices</h4>
        <ul>
            <li>do not guess across commitment boundaries (clarify or preview)</li>
            <li>show stable summaries that match commits</li>
            <li>treat consent as an artifact (IDs, scope, expiry), not chat text</li>
            <li>handle recovery deliberately (retry vs compensate vs escalate)</li>
            <li>propagate trace context and emit semantic events consistently</li>
        </ul>

        <h4>Middleware practices</h4>
        <ul>
            <li>central policy engine (cross-tool budgets/scopes/data rules)</li>
            <li>consent lifecycle service (grant, revoke, audit)</li>
            <li>OpenTelemetry-based trace pipeline with strict redaction rules</li>
            <li>automated error analysis (detect patterns of refusal, silent failure, misuse)</li>
        </ul>

        <div class="divider"></div>

        <h3>What to standardize first (minimal viable "community agreement")</h3>

        <p>If we want the smallest set that unlocks the most:</p>

        <ol>
            <li><strong>Action class + risk tier vocabulary</strong></li>
            <li><strong>Preview → Commit pattern</strong> (confirm tokens + idempotency)</li>
            <li><strong>Receipt schema</strong> (what changed + authority basis + correlation IDs)</li>
            <li><strong>Consent/delegation schema</strong> (scope/threshold/expiry/revocation + terms/policy version refs)</li>
            <li><strong>OpenTelemetry semantic conventions</strong> for agent-tool runs:
                <ul>
                    <li>tool invocation span</li>
                    <li>preview, approval requested/granted, refusal, commit, compensation events</li>
                    <li>receipt reference fields</li>
                </ul>
            </li>
        </ol>

        <p>Everything else can evolve around these.</p>

        <div class="divider"></div>

        <h3>Where MCP helps (and where it does not)</h3>

        <p><strong>MCP helps with:</strong></p>
        <ul>
            <li><strong>Local capability enumeration (per server)</strong> and consistent invocation structure</li>
            <li>a stable boundary where: policy hooks can be inserted, trace context can be propagated, validation and throttling can be applied (architecture-dependent)</li>
        </ul>

        <p><strong>MCP does not define:</strong></p>
        <ul>
            <li>capability semantics (meaning)</li>
            <li>guided flow semantics</li>
            <li>autonomy/authority contracts</li>
            <li>consent/terms artifacts</li>
            <li>receipts and business assertions</li>
            <li>semantic tracing conventions</li>
        </ul>

        <p>So the strategy is not "MCP solves governance". It is:</p>

        <blockquote>Use MCP as the substrate, then standardize the semantics and evidence on top.</blockquote>

        <div class="divider"></div>

        <h3>Closing thought</h3>

        <p>If we do this well, we get something the Web had implicitly:</p>
        <ul>
            <li>guided interaction,</li>
            <li>meaningful commitment,</li>
            <li>and a paper trail.</li>
        </ul>

        <p>But we make it explicit and machine-operable:</p>
        <ul>
            <li>authority becomes a contract,</li>
            <li>safety becomes enforceable,</li>
            <li>and responsibility becomes evidence-backed rather than narrative-driven.</li>
        </ul>

    </section>

</main>

<footer>
    AI Tools Integration — AI Design Course, University of Trento
    <br><span style="font-size: 0.85em;">Content by Fabio Casati · <a href="https://www.linkedin.com/in/sphoebs/">LinkedIn</a> · <a href="https://x.com/sphoebs">X</a></span>
</footer>

<script>
    hljs.highlightAll();

    // Highlight current section in nav
    const sections = document.querySelectorAll('section[id], .title-section');
    const navLinks = document.querySelectorAll('nav a');

    window.addEventListener('scroll', () => {
        let current = 'intro';
        sections.forEach(section => {
            const sectionTop = section.offsetTop;
            if (scrollY >= sectionTop - 200) {
                current = section.getAttribute('id') || 'intro';
            }
        });

        navLinks.forEach(link => {
            link.classList.remove('active');
            if (link.getAttribute('href') === '#' + current) {
                link.classList.add('active');
            }
        });
    });
</script>

</body>
</html>

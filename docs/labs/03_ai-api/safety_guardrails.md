# Safety Guardrails for AI Systems

*Security, safety, and privacy in tool-enabled AI*


## Learning Objectives

By the end of this reading, students should be able to:

- Explain why tool-enabled AI systems introduce **qualitatively different** security and safety risks compared to traditional software.

- Identify **confused deputy problems** that arise when untrusted inputs and outputs both influence control flow.

- Describe how **prompt injection** and **data exfiltration** attacks work in AI systems with tool access.

- Explain why **traditional API security** and input validation are insufficient for AI systems.

- Design **guards, policies, and isolation boundaries** at the AI system level.

- Identify **privacy risks** from data flowing through models and persisting in context or memory.


## 1. Why AI systems need different security thinking

TODO


## 2. The confused deputy problem, revisited

TODO
- Untrusted inputs influencing control flow
- Untrusted outputs injecting instructions
- Blurred trust boundaries


## 3. Prompt injection attacks

TODO
- Direct injection
- Indirect injection via tool outputs
- Examples and case studies


## 4. Data exfiltration via tools

TODO


## 5. Privacy concerns

TODO
- Data flowing through models without access control
- Information persisting in context
- Information persisting in memory


## 6. Why traditional defenses are insufficient

TODO
- API security
- Input validation
- Network boundaries


## 7. Designing guards and policies

TODO
- Isolation boundaries
- Policy enforcement
- Output filtering
- Sandboxing


## 8. Handling failure and uncertainty

TODO

